{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af348eba-b1ca-4b2b-a49a-ecd0cd2de78a",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3de6f-a30a-4a2e-91f4-593f1f41d88a",
   "metadata": {},
   "source": [
    "---\n",
    "In class today we will be implementing a Markov chain to process sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dda45-e3c8-43b4-8362-e43292a3c6c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "\n",
    "1. Students will be able to explain the Markov Chain process\n",
    "1. Implement a Markov Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7dfbe-f159-461e-a5ff-77a6218373be",
   "metadata": {},
   "source": [
    "---\n",
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4394c-9c65-4ed4-bfe8-0db56ec32223",
   "metadata": {},
   "source": [
    "Markov Chains represent a series of events following the Markov Property: future states are memory-less in that they depend only on the current state. This can be expanded to the idea of variable order Markov models where there is a variable-length memory (eg. 1st order Markov Model). Markov models consist of fully observable states. \n",
    "\n",
    "> A common example of this is in predicting the weather: We can clearly see the current weather and would like to predict tomorrow's weather. This is also applicable to biology with one case being CpG islands. \n",
    "\n",
    "Our goal today will be to implement a Markov model built from words. For our example text, we will use the classic example of Dr. Seuss because of the repetitive nature of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69269a05-73d5-47e4-96f3-29e25ed3b928",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Markov model\n",
    "\n",
    "For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: \"One fish two fish red fish blue fish.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a2e7a1-b676-4874-ad72-fe14edf98a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_model(markov_model, new_text):\n",
    "    '''\n",
    "    Function to build or add to a 1st order Markov model given a string of text\n",
    "    We will store the markov model as a dictionary of dictionaries\n",
    "    The key in the outer dictionary represents the current state\n",
    "    and the inner dictionary represents the next state with their contents containing\n",
    "    the transition probabilities.\n",
    "    Note: This would be easier to read if we were to build a class representation\n",
    "           of the model rather than a dictionary of dictionaries, but for simplicitiy\n",
    "           our implementation will just use this structure.\n",
    "    \n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "\n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated markov_model\n",
    "        \n",
    "    Pseudocode:\n",
    "        Add artificial states for start and end\n",
    "        For each word in text:\n",
    "            Increment markov_model[word][next_word]\n",
    "        \n",
    "    '''\n",
    "    def add_pair(first_pair,second_pair):\n",
    "        if(first_pair in markov_model):\n",
    "            markov_model[first_pair][second_pair] = 1\n",
    "        else:\n",
    "            markov_model[first_pair]={second_pair: 1}\n",
    "\n",
    "    text = new_text.split(\" \")\n",
    "    for count in range(0,len(text)):\n",
    "        if(count == 0):\n",
    "            add_pair('*S*',text[count])\n",
    "            add_pair(text[count],text[count+1])\n",
    "        elif(count == len(text)-1):\n",
    "            add_pair(text[count],'*E*')\n",
    "        else:\n",
    "            add_pair(text[count],text[count+1])\n",
    "    return markov_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc64609-ae18-4d8e-acd9-0e6a6110bf5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T02:49:14.042270Z",
     "start_time": "2025-01-18T02:49:14.035156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}\n"
     ]
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text)\n",
    "print (markov_model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7d42532-a75f-4ee4-aad0-fb9d6843050c",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36386041-90a1-4c7a-967c-ed08c38f56c0",
   "metadata": {},
   "source": [
    "###  Nth order Markov chain\n",
    "In the above model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. Here you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "796cf4ce-29e9-4507-a7e0-afc0e9b1429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_markov_model(markov_model, text, order=1):\n",
    "    '''\n",
    "    Function to build or add to a Nth order Markov model given a string of text\n",
    "\n",
    "    Args: \n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "            or None if a new model is being built\n",
    "        new_text (str): a string to build or add to the moarkov_model\n",
    "        order (int): the number of previous states to consider for the model\n",
    "        \n",
    "    Returns:\n",
    "        markov_model (dict of dicts): an updated/new markov_model\n",
    "    '''\n",
    "    def add_pair(first_pair,second_pair):\n",
    "        if(first_pair in markov_model):\n",
    "            markov_model[first_pair][second_pair] = 1\n",
    "        else:\n",
    "            markov_model[first_pair]={second_pair: 1}\n",
    "\n",
    "    this_text = text.split(\" \")\n",
    "    current_tuple = ('*S*',)*order\n",
    "    for count in range(0,len(this_text)+1):\n",
    "        if(count == 0):\n",
    "            add_pair(current_tuple,this_text[count])\n",
    "        elif(count == len(this_text)):\n",
    "            current_tuple = current_tuple[1:]+(this_text[count-1],)\n",
    "            add_pair(current_tuple,'*E*')\n",
    "        else:\n",
    "            current_tuple = current_tuple[1:]+(this_text[count-1],)\n",
    "            add_pair(current_tuple,this_text[count])\n",
    "    return markov_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62b8f110-6cad-41bb-b976-261a136282ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('*S*', '*S*'): {'one': 1},\n",
       " ('*S*', 'one'): {'fish': 1},\n",
       " ('one', 'fish'): {'two': 1},\n",
       " ('fish', 'two'): {'fish': 1},\n",
       " ('two', 'fish'): {'red': 1},\n",
       " ('fish', 'red'): {'fish': 1},\n",
       " ('red', 'fish'): {'blue': 1},\n",
       " ('fish', 'blue'): {'fish': 1},\n",
       " ('blue', 'fish'): {'*E*': 1}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_model = dict()\n",
    "text = \"one fish two fish red fish blue fish\"\n",
    "markov_model = build_markov_model(markov_model, text, order=2)\n",
    "markov_model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bf821ed-d84f-4fe7-9de9-3c21a3841aa3",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "{('*S*', '*S*'): {'one': 1},\n",
    " ('*S*', 'one'): {'fish': 1},\n",
    " ('one', 'fish'): {'two': 1},\n",
    " ('fish', 'two'): {'fish': 1},\n",
    " ('two', 'fish'): {'red': 1},\n",
    " ('fish', 'red'): {'fish': 1},\n",
    " ('red', 'fish'): {'blue': 1},\n",
    " ('fish', 'blue'): {'fish': 1},\n",
    " ('blue', 'fish'): {'*E*': 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14699827-2b59-4490-a961-a9ed04351a8e",
   "metadata": {},
   "source": [
    "## Generate text from Markov Model\n",
    "\n",
    "Markov models are \"generative models\". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.\n",
    "\n",
    "We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7447bbeadf1dcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_next_word(current_word, markov_model, seed=42):\n",
    "    '''\n",
    "    Function to randomly move a valid next state given a markov model\n",
    "    and a current state (word)\n",
    "\n",
    "    Args:\n",
    "        current_word (tuple): a word that exists in our model\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        next_word (str): a randomly selected next word based on transition probabilies\n",
    "\n",
    "    Pseudocode:\n",
    "        Calculate transition probilities for all next states from a given state (counts/sum)\n",
    "        Randomly draw from these to generate the next state\n",
    "\n",
    "    '''\n",
    "    word_list = []\n",
    "    freq_list = []\n",
    "    np.random.seed(seed)\n",
    "    for i in markov_model:\n",
    "\n",
    "        if current_word == i or current_word in i:  # handles both tuple or singular words\n",
    "            word_list.extend(markov_model[i].keys())\n",
    "            freq_list.extend(markov_model[i].values())\n",
    "    prob = [x / sum(freq_list) for x in freq_list]\n",
    "    random_word = np.random.choice(word_list, p=prob)\n",
    "    return random_word\n",
    "\n",
    "\n",
    "def generate_random_text(markov_model, seed=42):\n",
    "    '''\n",
    "    Function to generate text given a markov model\n",
    "\n",
    "    Args:\n",
    "        markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)\n",
    "\n",
    "    Returns:\n",
    "        sentence (str): a randomly generated sequence given the model\n",
    "\n",
    "    Pseudocode:\n",
    "        Initialize sentence at start state\n",
    "        Until End State:\n",
    "            append get_next_word(current_word, markov_model)\n",
    "        Return sentence\n",
    "\n",
    "    '''\n",
    "    scentence = []\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    order = 0\n",
    "    start_state = '*S*'\n",
    "    for i in markov_model:\n",
    "        if start_state in i:\n",
    "            order = len(i)\n",
    "            break\n",
    "\n",
    "    init_start_state = ('*S*',) * order\n",
    "\n",
    "    for _ in range(len(markov_model)):  # number of iteration found in the markov model\n",
    "        words_to_connect = (get_next_word(init_start_state, markov_model, seed))\n",
    "        if words_to_connect == '*E*':\n",
    "            break\n",
    "\n",
    "        scentence.append(words_to_connect)\n",
    "        init_start_state = (*init_start_state[1:], words_to_connect)\n",
    "    return ' '.join(scentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8724eb-5e10-4ffc-a76e-8a2015b3ee47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9280c1d-9f9e-47d8-8697-cc6255b3ec7d",
   "metadata": {},
   "source": [
    "## All the Fish\n",
    "Up till now, you have only been working with a line or two of the Dr. Seuss' _One Fish, Two Fish_. Now, I want you to build a model using the whole book and try different orders of Markov models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c04a137e-4c1d-40df-9056-177c022009ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One fish, Two fish, Red fish, Blue fish, Black fish, Blue fish, Old fish, New fish. This one has a little car. This one has a little star. Say! What a lot of fish there are. Yes. Some are red, and some are blue. Some are old and some are new. Some are sad, and some are glad, And some are very, very bad. Why are they sad and glad and bad? I do not know, go ask your dad. Some are thin, and some are fat. The fat one has a yellow hat. From there to here, From here to there, Funny things are everywhere. Here are some who like to run. They run for fun in the hot, hot sun. Oh me! Oh my! Oh me! oh my! What a lot of funny things go by. Some have two feet and some have four. Some have six feet and some have more. Where do they come from? I can't say. But I bet they have come a long, long way. we see them come, we see them go. Some are fast. Some are slow. Some are high. Some are low. Not one of them is like another. Don't ask us why, go ask your mother. Say! Look at his fingers! One, two, three... How many fingers do I see? One, two, three, four, five, six, seven, eight, nine, ten. He has eleven! Eleven! This is something new. I wish I had eleven too! Bump! Bump! Bump! Did you ever ride a Wump? We have a Wump with just one hump. But we know a man called Mr. Gump. Mr. gump has a seven hump Wump. So... If you like to go Bump! Bump! Just jump on the hump of the Wump of Gump Who am I? My name is Ned I do not like my little bed. This is no good. This is not right. My feet stick out of bed all night. And when I pull them in, Oh, Dear! My head sticks out of bed up here! We like our bike. It is made for three. Our Mike sits up in back, you see. We like our Mike, and this is why: Mike does all the work when the hills get high. Hello there, Ned. How do you do? Tell me, tell me what is new? How are things in your little bed? What is new? Please tell me Ned. I do not like this bed at all. a lot of things have come to call. A cow, a dog, a cat, a mouse. Oh! What a bed! Oh! What a house! Oh dear, oh dear! I cannot hear. Will you please come over near? Will you please look in my ear? There must be something there, I fear. Say look! A bird was in your ear. But he is out. So have no fear. Again your ear can hear, my dear. My hat is old, my teeth are gold. I have a bird I like to hold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is off, my foot is cold. My shoe is\n"
     ]
    }
   ],
   "source": [
    "# Now just add some more training data to the markov model. You can find it under data/one_fish_two_fish.txt\n",
    "\n",
    "markov_model = dict()\n",
    "# Read in the whole book\n",
    "# An example of a more complex text that we can use to generate more complex output\n",
    "\n",
    "file = open(\"data/one_fish_two_fish.txt\", \"r\")\n",
    "fishies = \"\"\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    fishies = fishies + ' ' + line\n",
    "markov_model = build_markov_model(markov_model, fishies, order=6)\n",
    "\n",
    "print (generate_random_text(markov_model,seed=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751e164-73d4-4c53-94d4-92f0d6f96b76",
   "metadata": {},
   "source": [
    "---\n",
    "## Shakespeare\n",
    "\n",
    "Now, let's play around with some Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1279dd44-0513-4cea-a51f-ac3b52927cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Then let not winter's ragged hand deface, In thee thy summer, ere thou be distill'd: Make sweet some vial; treasure thou some place With beauty's treasure ere it be self-kill'd. That use is not forbidden usury, Which happies those that pay the willing loan; That's for thy self thy foe, to thy sweet self too cruel: Thou that art now the world's fresh ornament, And only herald to the very same And that unfair which fairly doth excel; For never-resting time leads summer on To hideous winter, and confounds him there; Sap checked with frost, and lusty leaves quite gone, Beauty o'er-snowed and bareness every where: Then were not summer's distillation left, A liquid prisoner pent in walls of glass, Beauty's effect with beauty were bereft, Nor it, nor no remembrance what it was: But flowers distill'd, though they with winter meet, Leese but their show; their substance still lives sweet.\n"
     ]
    }
   ],
   "source": [
    "# An example of a more complex text that we can use to generate more complex output\n",
    "sonet_markov_model = dict()\n",
    "file = open(\"data/sonnets.txt\", \"r\")\n",
    "sonet = \"\"\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line == \"\":\n",
    "        # Empty line so build model\n",
    "        sonet_markov_model = build_markov_model(sonet_markov_model, sonet, order=2)\n",
    "        sonet = \"\"\n",
    "    else:\n",
    "        sonet = sonet + ' ' + line\n",
    " \n",
    "print (generate_random_text(sonet_markov_model,seed=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
